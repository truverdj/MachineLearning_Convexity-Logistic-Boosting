---
title: "STA 561 HW3 (Convexity, Logistic, Boosting)"
author: "Daniel Truver"
date: "2/7/2018"
header-includes:
  - \usepackage{amsmath}
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### (1) Linear Separability Implies Disjoint Convex Hulls

We proceed with proof by contradiction. 

For $\{x_n\}$ and $\{x_m'\}$, let their convex hulls be given by
$$
\begin{aligned}
& X = \{x: x = \sum_n\alpha_nx_x; ~ \alpha_n \geq0; ~\sum_n\alpha_n = 1 \} \\
& X' = \{x:x = \sum_m\alpha_m'x_m';~\alpha_m'\geq0;~\sum_m\alpha_m' = 1 \}
\end{aligned}
$$

(RAA) Suppose $\{x_n\}$ and $\{x_m'\}$ are linearly separable such that for a vector $w$ and scalar $w_0$ $w^Tx_n + w_0 >0,~ w^Tx_m' + w_0 < 0 ~\forall x_n, x_m'$ and $X\cap X' \neq \emptyset$.

Then $$\exists~ x \in X\cap X'$$
such that 
\begin{align}
x &= \sum_n\alpha_nx_n = \sum_m\alpha_m'x_m'\\
w^Tx &= w^T\sum_n\alpha_nx_n = w^T\sum_m\alpha_m'x_m'\\
w^Tx &= \sum_n\alpha_n(w^Tx_n) = \sum_m\alpha_m'(w^Tx_m') 
\end{align}

But, from linear separability, we have $w^Tx_n > -w_0$ and $w^Tx_m' < -w_0$. If we use these relations in the summations above, we obtain
$$
\begin{aligned}
& \sum_n\alpha_n(w^Tx_n) > \sum_n\alpha_n(-w_0), \quad  \sum_m\alpha_m'(w^Tx_m') < \sum_m\alpha_m'(-w_0) \\
& \implies \sum_n\alpha_n(w^Tx_n) > -w_0\sum_n\alpha_n, \quad \sum_m\alpha_m'(w^Tx_m') < -w_0\sum_m\alpha_m' \\
& \implies \sum_n\alpha_n(w^Tx_n) > -w_0, \quad \sum_m\alpha_m'(w^Tx_m') < -w_0 \\
& \implies w^Tx > -w_0 \quad\text{and}\quad w^Tx < -w_0 \quad\text{by (3)} \\
& ~\Rightarrow\!\Leftarrow
\end{aligned}
$$

Therefore, $X$ and $X'$ are disjoint; the convex hulls do not intersect. 

#### (2) Logistic Regression and Gradient Descent

##### (a) Logistic Sigmoid Function 

$$
\begin{aligned}
\sigma'(a) 
&= \frac{d}{da}(1+e^{-a})^{-1} \\
&= e^{-a}(1+e^{-a})^{-2} \\
&= (1+e^{-a})^{-1}\left(\frac{e^{-a}}{1+e^{-a}}  \right) \\
&= \sigma(a)\left( \frac{1+e^{-a}-1}{1+e^{-a}} \right) \\
& = \sigma(a)\left( 1- \frac{1}{1+e^{-a}} \right) \\
&= \sigma(a)(1-\sigma(a))
\end{aligned}
$$  

##### (b) Derivative of Cross Entropy 

$$
\begin{aligned}
\frac{\partial}{\partial w_j}L_w
&= \sum_{i = 1}^n \frac{\partial}{\partial w_j}\left(-y^{(i)}\log\sigma(w^Tx^{(i)})\right) - \frac{\partial}{\partial w_j}\left( (1- y^{(i)})\log(1-\sigma(w^Tx^{(i)})) \right) \\
&= \sum_{i = 1}^n -y^{(i)}\frac{1}{\sigma(w^Tx^{(i)})}\sigma(w^Tx^{(i)})(1-\sigma(w^Tx^{(i)}))(-x_j^{(i)})\\
&\quad \quad \quad - (1-y^{(i)})\frac{1}{1-\sigma(w^Tx^{(i)})}\left(-\sigma(w^Tx^{(i)})\right)\left(1-\sigma(w^T)\right)(-x_j^{(i)}) \\
&= \sum_{i=1}^n y^{(i)}x_j^{(i)}\left( 1-\sigma(w^Tx^{(i)}) \right) - (1-y^{(i)})x_j^{(i)}\sigma(w^Tx^{(i)}) \\
&= \sum_{i = 1}^n x_j^{(i)}\left( y^{(i)} \big( 1-\sigma(w^Tx^{(i)}) \big) - (1-y^{(i)})\sigma(w^Tx^{(i)}) \right)
\end{aligned}
$$  

##### (c) Convexity of the Cross Entropy Loss 

We want to show that, for $x, z \in \mathbb{R}^p$ and $\theta \in [0,1]$, 
$$
L_w(\theta x + (1-\theta)z) \leq \theta L(x) + (1-\theta)L(z).
$$

It suffices to show that $\forall i$, 
$$
g(x^{(i)}) = -y^{(i)}\log\sigma(w^Tx^{(i)}) -(1- y^{(i)})\log(1-\sigma(w^Tx^{(i)}))
$$
is convex.

We consider 2 cases, $y^{(i)}=1$ and $y^{(i)} =0$. 
Case 1 ($y^{(i)} =1$):
$$
g(x) = -\log\sigma(w^Tx) = \log(1+e^{-w^Tx})
$$

If we take the second derivative of this function in any direction $x_j$, we get
$$
\begin{aligned}
\frac{\partial^2}{\partial^2x_j}g(x) 
&= \frac{\partial}{\partial x_j}\left( -w_j e^{-w^Tx}(1+e^{-w^Tx})^{-1} \right) \\
&= w_j^2 e^{-w^Tx}(1+e^{-w^Tx})^{-1} - w_j^2(e^{-w^Tx})^2(1+e^{-w^Tx})^{-2} \\
&= w_j^2 e^{-w^Tx}(1+e^{-w^Tx})^{-1} (1- e^{-w^Tx}(1+e^{-w^Tx})^{-1}) \\
&\geq 0
\end{aligned}
$$

Therefore, this function is convex. 

Case 2 ($y^{(i)} = 0$):
$$
f(x) = -\log(1-\sigma(w^Tx))
$$
